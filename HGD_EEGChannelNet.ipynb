{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7f6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from analysis import *\n",
    "import argparse\n",
    "from sys import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c192d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "torch.cuda.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "torch.backends.cudnn.deterministics = True\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf1a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f52dd488700>\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff733115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv: image option\n",
    "length = 250\n",
    "channel = 128\n",
    "min_CNN = 200\n",
    "n_classes = 4\n",
    "classes = range(n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b68db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if platform == \"linux\" or platform == \"linux2\":\n",
    "#     torch_models_dir = r\"/media/titan/AI Research1/Data/CVPR2017\"\n",
    "# elif platform == \"win32\":\n",
    "#     torch_models_dir = r\"D:\\Data\\CVPR2021-02785\\CVPR2021-02785\\preprocessed\\torch_models\"\n",
    "# block_splits_all, block_splits_single, eeg_14_70, eeg_55_95, eeg_5_95, eeg_raw = os.listdir(torch_models_dir)\n",
    "# print(os.listdir(torch_models_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc401c6-e7e3-4163-9a3e-fa0aea23551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_dataset = '/media/mountHDD1/LanxHuyen/high_gamma_dataset.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd3ad4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_dataset = os.path.join(torch_models_dir, eeg_5_95)\n",
    "# splits_all_path = os.path.join(torch_models_dir, block_splits_all)\n",
    "# splits_single_path = os.path.join(torch_models_dir, block_splits_single)\n",
    "# # splits_path = os.path.join(torch_models_dir, splits_shuffled_path)\n",
    "# print(eeg_dataset,'\\n', splits_all_path, '\\n', splits_single_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "659d4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits_all = torch.load(splits_all_path)\n",
    "# splits_single = torch.load(splits_single_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3ef344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(splits_all['splits']))\n",
    "# print(len(splits_all['splits'][0]))\n",
    "\n",
    "# print(len(splits_all['splits'][5]['train']))\n",
    "# print(len(splits_all['splits'][5]['val']))\n",
    "# print(len(splits_all['splits'][5]['test']))\n",
    "# print(splits_all['splits'][0]['train'][:40])\n",
    "# print(splits_all['splits'][1]['train'][:40])\n",
    "# print(splits_all['splits'][2]['train'][:10])\n",
    "# print(splits_all['splits'][3]['train'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b27b3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(splits_single)\n",
    "# print(len(splits_single['splits'][0]['train']))\n",
    "# print(len(splits_single['splits'][0]['val']))\n",
    "# print(len(splits_single['splits'][0]['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed58c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_loaded = torch.load(eeg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a637c6f-a7c4-4689-9036-0b1f0089ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(eeg_loaded)))\n",
    "random.shuffle(list(range(len(eeg_loaded))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "682d14b3-76a3-456e-99eb-bf75d423c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(eeg_loaded))\n",
    "val_size = int(0.1 * len(eeg_loaded))\n",
    "test_size = len(eeg_loaded) - (train_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6cd3e5f-d47b-4cb4-ad42-1f46608eb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size+val_size]\n",
    "test_indices = indices[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f3026ea-0a7a-4031-9a3d-e3e3540254f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_splits = {\"train\": train_indices, \"val\": val_indices, \"test\": test_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89460bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26968\n",
      "26968\n"
     ]
    }
   ],
   "source": [
    "print(len(eeg_loaded))\n",
    "# print(eeg_loaded.keys())\n",
    "eeg = [eeg_loaded[i][\"eeg\"] for i in range (len(eeg_loaded))]\n",
    "print(len(eeg))\n",
    "# for dataset_idx in range (len(eeg_loaded)): \n",
    "#     eeg, label = [eeg_loaded[dataset_idx][key] for key in ['eeg', 'label']]\n",
    "# print((label))\n",
    "# print(len(eeg))\n",
    "# # print(len(dataset))\n",
    "\n",
    "# # print(labels)\n",
    "# # print(images[0])\n",
    "# print(eeg['eeg'][0].shape)\n",
    "# print(eeg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d20c958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "opt = {\n",
    "    # Dataset options\n",
    "#     \"iv\": \"image\",\n",
    "#     \"offset\": None,\n",
    "    \"results_file\": \"results.pkl\",\n",
    "    #\"subject\": 0,\n",
    "    \"time_low\": 20,\n",
    "    \"time_high\": 270,\n",
    "#     \"run\": \"none\",\n",
    "    \"eeg_dataset\": eeg_dataset,\n",
    "    \"model_type\": \"model10\",\n",
    "    #\"splits_path\": splits_all_path,\n",
    "    #\"split_num\": 0,\n",
    "    \"split_name\": \"train\",\n",
    "#     \"fold\": 5,\n",
    "    #Training options\n",
    "    \"batch_size\": 64,\n",
    "    \"optim\": \"Adam\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"learning_rate_decay_by\": 0.5,\n",
    "    \"learning_rate_decay_every\": 10,\n",
    "    \"epochs\": 100,\n",
    "    \"GPUindex\": 0,\n",
    "    \"kind\":\"from-scratch\",\n",
    "    #Backend options\n",
    "    \"no_cuda\": False,\n",
    "    \"classifier\": None\n",
    "}\n",
    "opt = argparse.Namespace(**opt)\n",
    "print(opt.time_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce8d6e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.utils.data import DataLoader\n",
    "# from data_loader import EEGDataset, Splitter, SplitterWithData\n",
    "from data_loader_HGD import EEGDataset, Splitter\n",
    "from EEG_Encoder.LSTM import classifier_LSTM\n",
    "from EEG_Encoder.CNN import classifier_CNN\n",
    "from EEG_Encoder.EEGNet import classifier_EEGNet\n",
    "from EEG_Encoder.SyncNet import classifier_SyncNet\n",
    "from EEG_Encoder.EEGChannelNet import classifier_EEGChannelNet\n",
    "from EEG_Encoder.net_generator import Classifier\n",
    "from EEG_Encoder.net_trainer import net_trainer\n",
    "from p_values import *\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a5b475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "#              offset,\n",
    "             eeg_dataset,\n",
    "             #splits_path,\n",
    "             #split_num, # (0-5) - 6 fold cross validation\n",
    "             split_name,\n",
    "#              total, \n",
    "#              classes,\n",
    "#              classifier,\n",
    "             batch_size,\n",
    "#              GPUindex,\n",
    "#              length, # 500\n",
    "#              channel, # 128\n",
    "#              min_CNN,\n",
    "             opt,\n",
    "             kind=\"from-scratch\"):        \n",
    "    # Load dataset\n",
    "    dataset = EEGDataset(opt, eeg_dataset)\n",
    "    print(\"DONE: LOAD DATASET\")\n",
    "#     # Create loaders for LSTM/MLP/CNN/SCNN/EEGNet/SyncNet/EEGChannelNet\n",
    "#     if kind==\"from-scratch\":\n",
    "#         relabel = False\n",
    "#     if kind==\"incremental\":\n",
    "#         relabel = False\n",
    "#     if kind==\"no-model-file\":\n",
    "#         relabel = True\n",
    "    splitter = {split: Splitter(dataset,\n",
    "                    #splits_path,\n",
    "                    #split_num,\n",
    "                    split_name = split) \n",
    "                for split in [\"train\", \"val\", \"test\"]\n",
    "                                }\n",
    "    loaders = {split: DataLoader(\n",
    "                        splitter[split],\n",
    "                        batch_size = batch_size,\n",
    "                        drop_last = False,\n",
    "                        shuffle = True)\n",
    "                    for split in [\"train\", \"val\", \"test\"]}\n",
    "    channel_idx = None    \n",
    "    print(\"DONE: Create loaders for model\")            \n",
    "    return dataset, loaders, splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a242cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "opt.classifier = \"EEGChannelNet\"\n",
    "opt.batch_size = 64\n",
    "# opt.kind = \"from-scratch\"\n",
    "# opt.run = \"imagenet40-1000\"\n",
    "# opt.fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "633488a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: LOAD DATASET\n",
      "DONE: Create loaders for model\n"
     ]
    }
   ],
   "source": [
    "dataset, loaders, splitter = load_dataset(\n",
    "#              offset,\n",
    "             opt.eeg_dataset,\n",
    "             #opt.splits_path,\n",
    "             #opt.split_num, # (0-5) - 6 fold cross validation\n",
    "             opt.split_name,\n",
    "#              total, \n",
    "#              classes,\n",
    "#              classifier,\n",
    "             opt.batch_size,\n",
    "#              GPUindex,\n",
    "#              length, # 500\n",
    "#              channel, # 128\n",
    "#              min_CNN,\n",
    "             opt,\n",
    "             opt.kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e99fc973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'data_loader_HGD.EEGDataset'>\n",
      "<class 'dict'>\n",
      "3 [338, 43, 43]\n",
      "1: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "2: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "3: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "4: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "5: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "6: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "7: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "8: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "9: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "10: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "11: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "12: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "13: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "14: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "15: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "16: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "17: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "18: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "19: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n",
      "20: Target size: torch.Size([64]); input size: torch.Size([64, 128, 250])\n"
     ]
    }
   ],
   "source": [
    "# loaders: divide the splits data in each fold with batch_size\n",
    "# Each fold has {train: 8000 idx, val: 2000 idx, test: 2000 idx}\n",
    "# Each loader batch has {train: 2000 idx, val: 250 idx, test: 250 idx}\n",
    "print(type(dataset))\n",
    "print(type(loaders))\n",
    "print(len(loaders), [len(loaders[name]) for name in [\"train\", \"val\", \"test\"] ])\n",
    "for i, (input, target) in enumerate(loaders[\"train\"]):\n",
    "    if i<20:\n",
    "        print(f\"{i+1}: Target size: {target.size()}; input size: {input.size()}\")\n",
    "# for i in range(0, 40):\n",
    "#     eeg, label_val = splitter[\"val\"][i]\n",
    "#     eeg, label_train = splitter[\"train\"][i]\n",
    "#     print(f\"{i+1}: Label val: {label_val}; label train: {label_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39a07cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: CREATE TORCH CLASSIFIER\n",
      "classifier_EEGChannelNet(\n",
      "  (encoder): FeaturesExtractor(\n",
      "    (temporal_block): TemporalBlock(\n",
      "      (layers): ModuleList(\n",
      "        (0): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(1, 10, kernel_size=(1, 33), stride=(1, 2), padding=(0, 16))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(1, 10, kernel_size=(1, 33), stride=(1, 2), padding=(0, 32), dilation=(1, 2))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "        (2): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(1, 10, kernel_size=(1, 33), stride=(1, 2), padding=(0, 64), dilation=(1, 4))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "        (3): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(1, 10, kernel_size=(1, 33), stride=(1, 2), padding=(0, 128), dilation=(1, 8))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (spatial_block): SpatialBlock(\n",
      "      (layers): ModuleList(\n",
      "        (0): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(40, 50, kernel_size=(128, 1), stride=(2, 1), padding=(63, 0))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(40, 50, kernel_size=(64, 1), stride=(2, 1), padding=(31, 0))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "        (2): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(40, 50, kernel_size=(42, 1), stride=(2, 1), padding=(20, 0))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "        (3): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(40, 50, kernel_size=(32, 1), stride=(2, 1), padding=(15, 0))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res_blocks): ModuleList(\n",
      "      (0-3): 4 x Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (conv1): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (norm): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv): Conv2d(200, 200, kernel_size=(3, 3), stride=(2, 2))\n",
      "          (drop): Dropout2d(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_conv): ConvLayer2D(\n",
      "      (norm): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(200, 50, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (drop): Dropout2d(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=1000, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=1000, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "classifier_EEGChannelNet                      [1, 4]                    --\n",
       "├─FeaturesExtractor: 1-1                      [1, 50, 1, 4]             --\n",
       "│    └─TemporalBlock: 2-1                     [1, 40, 128, 125]         --\n",
       "│    │    └─ModuleList: 3-1                   --                        1,368\n",
       "│    └─SpatialBlock: 2-2                      [1, 200, 64, 125]         --\n",
       "│    │    └─ModuleList: 3-2                   --                        532,520\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─Sequential: 3-3                   [1, 200, 31, 62]          1,081,400\n",
       "│    │    └─Sequential: 3-4                   [1, 200, 15, 30]          1,081,400\n",
       "│    │    └─Sequential: 3-5                   [1, 200, 7, 14]           1,081,400\n",
       "│    │    └─Sequential: 3-6                   [1, 200, 3, 6]            1,081,400\n",
       "│    └─ConvLayer2D: 2-4                       [1, 50, 1, 4]             --\n",
       "│    │    └─BatchNorm2d: 3-7                  [1, 200, 3, 6]            400\n",
       "│    │    └─ReLU: 3-8                         [1, 200, 3, 6]            --\n",
       "│    │    └─Conv2d: 3-9                       [1, 50, 1, 4]             90,050\n",
       "│    │    └─Dropout2d: 3-10                   [1, 50, 1, 4]             --\n",
       "├─Sequential: 1-2                             [1, 4]                    --\n",
       "│    └─Linear: 2-5                            [1, 1000]                 201,000\n",
       "│    └─ReLU: 2-6                              [1, 1000]                 --\n",
       "│    └─Linear: 2-7                            [1, 4]                    4,004\n",
       "===============================================================================================\n",
       "Total params: 5,154,942\n",
       "Trainable params: 5,154,942\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.71\n",
       "===============================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 127.20\n",
       "Params size (MB): 20.62\n",
       "Estimated Total Size (MB): 147.95\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net, nonclasses = Classifier(\n",
    "                 n_classes,\n",
    "                 classes,\n",
    "                 opt.classifier,\n",
    "                 opt.GPUindex,\n",
    "                 length,\n",
    "                 channel,\n",
    "                 min_CNN,\n",
    "                 opt.kind)\n",
    "# print(len(nonclasses))\n",
    "summary(net, input_size=(1,128, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a8d6995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGChannelNetHGD\n",
      "results_EEGChannelNetHGD\n"
     ]
    }
   ],
   "source": [
    "model_path = (   opt.classifier+\n",
    "                  \"HGD\" )\n",
    "results_path = (  \"results_\"+ opt.classifier+\n",
    "                  \"HGD\"  )\n",
    "print(model_path)\n",
    "print(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b753db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(results_file='results.pkl', time_low=20, time_high=270, eeg_dataset='/media/mountHDD1/LanxHuyen/high_gamma_dataset.pth', model_type='model10', split_name='train', batch_size=64, optim='Adam', learning_rate=0.001, learning_rate_decay_by=0.5, learning_rate_decay_every=10, epochs=100, GPUindex=0, kind='from-scratch', no_cuda=False, classifier='EEGChannelNet')\n"
     ]
    }
   ],
   "source": [
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5334cb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Batch 100 (every 100 batch): Loss=1.3797; accuracy=0.3130\n",
      "Train Batch 200 (every 100 batch): Loss=1.2786; accuracy=0.3315\n",
      "Train Batch 300 (every 100 batch): Loss=1.2059; accuracy=0.3429\n",
      "Epoch 1 summary: train_loss: 1.3171 | train_acc: 0.3472 | val_loss: 1.2475 | val_acc: 0.3946\n",
      "Epoch 2\n",
      "Train Batch 100 (every 100 batch): Loss=1.2090; accuracy=0.3947\n",
      "Train Batch 200 (every 100 batch): Loss=1.2798; accuracy=0.3980\n",
      "Train Batch 300 (every 100 batch): Loss=1.1599; accuracy=0.4015\n",
      "Epoch 2 summary: train_loss: 1.2373 | train_acc: 0.4019 | val_loss: 1.1809 | val_acc: 0.4310\n",
      "Epoch 3\n",
      "Train Batch 100 (every 100 batch): Loss=1.2690; accuracy=0.4087\n",
      "Train Batch 200 (every 100 batch): Loss=1.2772; accuracy=0.4119\n",
      "Train Batch 300 (every 100 batch): Loss=1.2901; accuracy=0.4136\n",
      "Epoch 3 summary: train_loss: 1.2101 | train_acc: 0.4150 | val_loss: 1.1488 | val_acc: 0.4488\n",
      "Epoch 4\n",
      "Train Batch 100 (every 100 batch): Loss=1.1733; accuracy=0.4173\n",
      "Train Batch 200 (every 100 batch): Loss=1.1290; accuracy=0.4284\n",
      "Train Batch 300 (every 100 batch): Loss=1.1816; accuracy=0.4322\n",
      "Epoch 4 summary: train_loss: 1.1829 | train_acc: 0.4321 | val_loss: 1.1317 | val_acc: 0.4520\n",
      "Epoch 5\n",
      "Train Batch 100 (every 100 batch): Loss=1.1845; accuracy=0.4417\n",
      "Train Batch 200 (every 100 batch): Loss=1.0590; accuracy=0.4438\n",
      "Train Batch 300 (every 100 batch): Loss=1.0855; accuracy=0.4440\n",
      "Epoch 5 summary: train_loss: 1.1664 | train_acc: 0.4441 | val_loss: 1.1617 | val_acc: 0.4535\n",
      "Epoch 6\n",
      "Train Batch 100 (every 100 batch): Loss=1.1243; accuracy=0.4636\n",
      "Train Batch 200 (every 100 batch): Loss=1.0701; accuracy=0.4709\n",
      "Train Batch 300 (every 100 batch): Loss=1.0256; accuracy=0.4772\n",
      "Epoch 6 summary: train_loss: 1.1347 | train_acc: 0.4773 | val_loss: 1.1222 | val_acc: 0.4829\n",
      "Epoch 7\n",
      "Train Batch 100 (every 100 batch): Loss=0.9972; accuracy=0.5078\n",
      "Train Batch 200 (every 100 batch): Loss=1.0632; accuracy=0.5114\n",
      "Train Batch 300 (every 100 batch): Loss=1.1070; accuracy=0.5159\n",
      "Epoch 7 summary: train_loss: 1.0863 | train_acc: 0.5192 | val_loss: 1.0879 | val_acc: 0.5280\n",
      "Epoch 8\n",
      "Train Batch 100 (every 100 batch): Loss=1.0406; accuracy=0.5442\n",
      "Train Batch 200 (every 100 batch): Loss=1.0020; accuracy=0.5551\n",
      "Train Batch 300 (every 100 batch): Loss=1.0558; accuracy=0.5602\n",
      "Epoch 8 summary: train_loss: 1.0267 | train_acc: 0.5617 | val_loss: 1.0391 | val_acc: 0.5603\n",
      "Epoch 9\n",
      "Train Batch 100 (every 100 batch): Loss=1.0067; accuracy=0.5903\n",
      "Train Batch 200 (every 100 batch): Loss=0.9206; accuracy=0.5923\n",
      "Train Batch 300 (every 100 batch): Loss=0.8190; accuracy=0.5958\n",
      "Epoch 9 summary: train_loss: 0.9632 | train_acc: 0.5960 | val_loss: 1.0725 | val_acc: 0.5425\n",
      "Epoch 10\n",
      "Train Batch 100 (every 100 batch): Loss=0.9731; accuracy=0.6184\n",
      "Train Batch 200 (every 100 batch): Loss=0.9453; accuracy=0.6244\n",
      "Train Batch 300 (every 100 batch): Loss=0.9139; accuracy=0.6239\n",
      "Epoch 10 summary: train_loss: 0.9066 | train_acc: 0.6257 | val_loss: 0.9887 | val_acc: 0.5952\n",
      "Epoch 11\n",
      "Train Batch 100 (every 100 batch): Loss=0.8170; accuracy=0.6448\n",
      "Train Batch 200 (every 100 batch): Loss=0.8433; accuracy=0.6449\n",
      "Train Batch 300 (every 100 batch): Loss=0.9243; accuracy=0.6482\n",
      "Epoch 11 summary: train_loss: 0.8575 | train_acc: 0.6500 | val_loss: 1.0311 | val_acc: 0.5810\n",
      "Epoch 12\n",
      "Train Batch 100 (every 100 batch): Loss=0.8256; accuracy=0.6697\n",
      "Train Batch 200 (every 100 batch): Loss=0.5843; accuracy=0.6667\n",
      "Train Batch 300 (every 100 batch): Loss=0.7043; accuracy=0.6691\n",
      "Epoch 12 summary: train_loss: 0.8196 | train_acc: 0.6675 | val_loss: 1.0746 | val_acc: 0.5680\n",
      "Epoch 13\n",
      "Train Batch 100 (every 100 batch): Loss=0.9090; accuracy=0.6858\n",
      "Train Batch 200 (every 100 batch): Loss=0.7516; accuracy=0.6895\n",
      "Train Batch 300 (every 100 batch): Loss=0.7174; accuracy=0.6866\n",
      "Epoch 13 summary: train_loss: 0.7846 | train_acc: 0.6874 | val_loss: 1.0659 | val_acc: 0.5709\n",
      "Epoch 14\n",
      "Train Batch 100 (every 100 batch): Loss=0.8358; accuracy=0.7078\n",
      "Train Batch 200 (every 100 batch): Loss=0.8464; accuracy=0.7065\n",
      "Train Batch 300 (every 100 batch): Loss=0.7587; accuracy=0.7034\n",
      "Epoch 14 summary: train_loss: 0.7457 | train_acc: 0.7040 | val_loss: 1.0629 | val_acc: 0.5916\n",
      "Epoch 15\n",
      "Train Batch 100 (every 100 batch): Loss=0.8168; accuracy=0.7255\n",
      "Train Batch 200 (every 100 batch): Loss=0.9737; accuracy=0.7221\n",
      "Train Batch 300 (every 100 batch): Loss=0.6828; accuracy=0.7216\n",
      "Epoch 15 summary: train_loss: 0.7041 | train_acc: 0.7200 | val_loss: 1.0187 | val_acc: 0.5865\n",
      "Epoch 16\n",
      "Train Batch 100 (every 100 batch): Loss=0.6745; accuracy=0.7252\n",
      "Train Batch 200 (every 100 batch): Loss=0.6949; accuracy=0.7344\n",
      "Train Batch 300 (every 100 batch): Loss=0.5295; accuracy=0.7342\n",
      "Epoch 16 summary: train_loss: 0.6769 | train_acc: 0.7329 | val_loss: 1.0047 | val_acc: 0.6050\n",
      "Epoch 17\n",
      "Train Batch 100 (every 100 batch): Loss=0.6506; accuracy=0.7412\n",
      "Train Batch 200 (every 100 batch): Loss=0.5081; accuracy=0.7480\n",
      "Train Batch 300 (every 100 batch): Loss=0.5568; accuracy=0.7497\n",
      "Epoch 17 summary: train_loss: 0.6375 | train_acc: 0.7500 | val_loss: 1.0937 | val_acc: 0.5774\n",
      "Epoch 18\n",
      "Train Batch 100 (every 100 batch): Loss=0.8157; accuracy=0.7741\n",
      "Train Batch 200 (every 100 batch): Loss=0.5417; accuracy=0.7729\n",
      "Train Batch 300 (every 100 batch): Loss=0.6788; accuracy=0.7691\n",
      "Epoch 18 summary: train_loss: 0.6003 | train_acc: 0.7686 | val_loss: 1.0515 | val_acc: 0.6097\n",
      "Epoch 19\n",
      "Train Batch 100 (every 100 batch): Loss=0.5995; accuracy=0.7877\n",
      "Train Batch 200 (every 100 batch): Loss=0.5034; accuracy=0.7813\n",
      "Train Batch 300 (every 100 batch): Loss=0.4323; accuracy=0.7824\n",
      "Epoch 19 summary: train_loss: 0.5676 | train_acc: 0.7797 | val_loss: 1.0779 | val_acc: 0.6068\n",
      "Epoch 20\n",
      "Train Batch 100 (every 100 batch): Loss=0.5070; accuracy=0.7969\n",
      "Train Batch 200 (every 100 batch): Loss=0.4780; accuracy=0.7945\n",
      "Train Batch 300 (every 100 batch): Loss=0.5003; accuracy=0.7899\n",
      "Epoch 20 summary: train_loss: 0.5483 | train_acc: 0.7885 | val_loss: 1.1014 | val_acc: 0.6079\n",
      "Epoch 21\n",
      "Train Batch 100 (every 100 batch): Loss=0.4579; accuracy=0.8044\n",
      "Train Batch 200 (every 100 batch): Loss=0.5936; accuracy=0.8060\n",
      "Train Batch 300 (every 100 batch): Loss=0.5655; accuracy=0.8068\n",
      "Epoch 21 summary: train_loss: 0.5044 | train_acc: 0.8056 | val_loss: 1.1542 | val_acc: 0.5847\n",
      "Epoch 22\n",
      "Train Batch 100 (every 100 batch): Loss=0.4930; accuracy=0.8255\n",
      "Train Batch 200 (every 100 batch): Loss=0.3181; accuracy=0.8268\n",
      "Train Batch 300 (every 100 batch): Loss=0.3977; accuracy=0.8242\n",
      "Epoch 22 summary: train_loss: 0.4664 | train_acc: 0.8226 | val_loss: 1.1815 | val_acc: 0.5905\n",
      "Epoch 23\n",
      "Train Batch 100 (every 100 batch): Loss=0.5084; accuracy=0.8423\n",
      "Train Batch 200 (every 100 batch): Loss=0.5357; accuracy=0.8356\n",
      "Train Batch 300 (every 100 batch): Loss=0.5160; accuracy=0.8340\n",
      "Epoch 23 summary: train_loss: 0.4369 | train_acc: 0.8326 | val_loss: 1.3026 | val_acc: 0.5901\n",
      "Epoch 24\n",
      "Train Batch 100 (every 100 batch): Loss=0.2395; accuracy=0.8333\n",
      "Train Batch 200 (every 100 batch): Loss=0.5199; accuracy=0.8380\n",
      "Train Batch 300 (every 100 batch): Loss=0.3742; accuracy=0.8389\n",
      "Epoch 24 summary: train_loss: 0.4258 | train_acc: 0.8380 | val_loss: 1.3268 | val_acc: 0.5959\n",
      "Epoch 25\n",
      "Train Batch 100 (every 100 batch): Loss=0.4552; accuracy=0.8522\n",
      "Train Batch 200 (every 100 batch): Loss=0.3709; accuracy=0.8472\n",
      "Train Batch 300 (every 100 batch): Loss=0.4049; accuracy=0.8447\n",
      "Epoch 25 summary: train_loss: 0.4028 | train_acc: 0.8474 | val_loss: 1.3599 | val_acc: 0.5854\n",
      "Epoch 26\n",
      "Train Batch 100 (every 100 batch): Loss=0.4218; accuracy=0.8550\n",
      "Train Batch 200 (every 100 batch): Loss=0.3972; accuracy=0.8556\n",
      "Train Batch 300 (every 100 batch): Loss=0.3340; accuracy=0.8547\n",
      "Epoch 26 summary: train_loss: 0.3841 | train_acc: 0.8543 | val_loss: 1.2894 | val_acc: 0.5872\n",
      "Epoch 27\n",
      "Train Batch 100 (every 100 batch): Loss=0.1788; accuracy=0.8637\n",
      "Train Batch 200 (every 100 batch): Loss=0.4745; accuracy=0.8659\n",
      "Train Batch 300 (every 100 batch): Loss=0.3362; accuracy=0.8655\n",
      "Epoch 27 summary: train_loss: 0.3589 | train_acc: 0.8640 | val_loss: 1.3309 | val_acc: 0.5894\n",
      "Epoch 28\n",
      "Train Batch 100 (every 100 batch): Loss=0.3200; accuracy=0.8773\n",
      "Train Batch 200 (every 100 batch): Loss=0.3799; accuracy=0.8740\n",
      "Train Batch 300 (every 100 batch): Loss=0.4812; accuracy=0.8727\n",
      "Epoch 28 summary: train_loss: 0.3447 | train_acc: 0.8721 | val_loss: 1.3387 | val_acc: 0.5898\n",
      "Epoch 29\n",
      "Train Batch 100 (every 100 batch): Loss=0.2202; accuracy=0.8836\n",
      "Train Batch 200 (every 100 batch): Loss=0.4216; accuracy=0.8837\n",
      "Train Batch 300 (every 100 batch): Loss=0.3042; accuracy=0.8819\n",
      "Epoch 29 summary: train_loss: 0.3227 | train_acc: 0.8793 | val_loss: 1.3845 | val_acc: 0.6079\n",
      "Epoch 30\n",
      "Train Batch 100 (every 100 batch): Loss=0.2627; accuracy=0.8939\n",
      "Train Batch 200 (every 100 batch): Loss=0.1727; accuracy=0.8888\n",
      "Train Batch 300 (every 100 batch): Loss=0.4366; accuracy=0.8860\n",
      "Epoch 30 summary: train_loss: 0.3138 | train_acc: 0.8850 | val_loss: 1.4409 | val_acc: 0.5908\n",
      "Epoch 31\n",
      "Train Batch 100 (every 100 batch): Loss=0.3843; accuracy=0.8808\n",
      "Train Batch 200 (every 100 batch): Loss=0.1968; accuracy=0.8849\n",
      "Train Batch 300 (every 100 batch): Loss=0.5032; accuracy=0.8840\n",
      "Epoch 31 summary: train_loss: 0.3063 | train_acc: 0.8842 | val_loss: 1.4417 | val_acc: 0.5901\n",
      "Epoch 32\n",
      "Train Batch 100 (every 100 batch): Loss=0.2171; accuracy=0.9016\n",
      "Train Batch 200 (every 100 batch): Loss=0.2426; accuracy=0.8994\n",
      "Train Batch 300 (every 100 batch): Loss=0.2086; accuracy=0.8977\n",
      "Epoch 32 summary: train_loss: 0.2788 | train_acc: 0.8961 | val_loss: 1.4166 | val_acc: 0.5970\n",
      "Epoch 33\n",
      "Train Batch 100 (every 100 batch): Loss=0.2853; accuracy=0.8964\n",
      "Train Batch 200 (every 100 batch): Loss=0.2546; accuracy=0.8958\n",
      "Train Batch 300 (every 100 batch): Loss=0.1642; accuracy=0.8958\n",
      "Epoch 33 summary: train_loss: 0.2838 | train_acc: 0.8950 | val_loss: 1.3408 | val_acc: 0.6007\n",
      "Epoch 34\n",
      "Train Batch 100 (every 100 batch): Loss=0.4793; accuracy=0.9089\n",
      "Train Batch 200 (every 100 batch): Loss=0.1506; accuracy=0.9086\n",
      "Train Batch 300 (every 100 batch): Loss=0.3473; accuracy=0.9073\n",
      "Epoch 34 summary: train_loss: 0.2550 | train_acc: 0.9068 | val_loss: 1.5148 | val_acc: 0.6017\n",
      "Epoch 35\n",
      "Train Batch 100 (every 100 batch): Loss=0.2612; accuracy=0.9117\n",
      "Train Batch 200 (every 100 batch): Loss=0.1946; accuracy=0.9081\n",
      "Train Batch 300 (every 100 batch): Loss=0.2641; accuracy=0.9062\n",
      "Epoch 35 summary: train_loss: 0.2531 | train_acc: 0.9065 | val_loss: 1.4855 | val_acc: 0.5977\n",
      "Epoch 36\n",
      "Train Batch 100 (every 100 batch): Loss=0.1579; accuracy=0.9148\n",
      "Train Batch 200 (every 100 batch): Loss=0.3271; accuracy=0.9140\n",
      "Train Batch 300 (every 100 batch): Loss=0.0816; accuracy=0.9129\n",
      "Epoch 36 summary: train_loss: 0.2456 | train_acc: 0.9124 | val_loss: 1.5484 | val_acc: 0.5981\n",
      "Epoch 37\n",
      "Train Batch 100 (every 100 batch): Loss=0.2234; accuracy=0.9156\n",
      "Train Batch 200 (every 100 batch): Loss=0.2992; accuracy=0.9184\n",
      "Train Batch 300 (every 100 batch): Loss=0.1903; accuracy=0.9169\n",
      "Epoch 37 summary: train_loss: 0.2258 | train_acc: 0.9177 | val_loss: 1.5708 | val_acc: 0.6079\n",
      "Epoch 38\n",
      "Train Batch 100 (every 100 batch): Loss=0.1446; accuracy=0.9170\n",
      "Train Batch 200 (every 100 batch): Loss=0.1644; accuracy=0.9207\n",
      "Train Batch 300 (every 100 batch): Loss=0.3101; accuracy=0.9204\n",
      "Epoch 38 summary: train_loss: 0.2265 | train_acc: 0.9186 | val_loss: 1.4265 | val_acc: 0.5988\n",
      "Epoch 39\n",
      "Train Batch 100 (every 100 batch): Loss=0.2282; accuracy=0.9212\n",
      "Train Batch 200 (every 100 batch): Loss=0.2900; accuracy=0.9203\n",
      "Train Batch 300 (every 100 batch): Loss=0.2729; accuracy=0.9205\n",
      "Epoch 39 summary: train_loss: 0.2221 | train_acc: 0.9214 | val_loss: 1.6424 | val_acc: 0.5872\n",
      "Epoch 40\n",
      "Train Batch 100 (every 100 batch): Loss=0.3313; accuracy=0.9245\n",
      "Train Batch 200 (every 100 batch): Loss=0.1307; accuracy=0.9250\n",
      "Train Batch 300 (every 100 batch): Loss=0.1103; accuracy=0.9265\n",
      "Epoch 40 summary: train_loss: 0.2055 | train_acc: 0.9260 | val_loss: 1.5463 | val_acc: 0.5821\n",
      "Epoch 41\n",
      "Train Batch 100 (every 100 batch): Loss=0.0872; accuracy=0.9311\n",
      "Train Batch 200 (every 100 batch): Loss=0.2288; accuracy=0.9287\n",
      "Train Batch 300 (every 100 batch): Loss=0.1537; accuracy=0.9282\n",
      "Epoch 41 summary: train_loss: 0.1991 | train_acc: 0.9271 | val_loss: 1.6060 | val_acc: 0.5876\n",
      "Epoch 42\n",
      "Train Batch 100 (every 100 batch): Loss=0.1051; accuracy=0.9387\n",
      "Train Batch 200 (every 100 batch): Loss=0.0699; accuracy=0.9364\n",
      "Train Batch 300 (every 100 batch): Loss=0.1938; accuracy=0.9336\n",
      "Epoch 42 summary: train_loss: 0.1895 | train_acc: 0.9326 | val_loss: 1.5930 | val_acc: 0.5861\n",
      "Epoch 43\n",
      "Train Batch 100 (every 100 batch): Loss=0.2184; accuracy=0.9291\n",
      "Train Batch 200 (every 100 batch): Loss=0.1957; accuracy=0.9300\n",
      "Train Batch 300 (every 100 batch): Loss=0.2433; accuracy=0.9320\n",
      "Epoch 43 summary: train_loss: 0.1914 | train_acc: 0.9317 | val_loss: 1.5717 | val_acc: 0.5858\n",
      "Epoch 44\n",
      "Train Batch 100 (every 100 batch): Loss=0.0860; accuracy=0.9411\n",
      "Train Batch 200 (every 100 batch): Loss=0.1639; accuracy=0.9401\n",
      "Train Batch 300 (every 100 batch): Loss=0.1287; accuracy=0.9374\n",
      "Epoch 44 summary: train_loss: 0.1773 | train_acc: 0.9370 | val_loss: 1.7608 | val_acc: 0.5661\n",
      "Epoch 45\n",
      "Train Batch 100 (every 100 batch): Loss=0.0843; accuracy=0.9403\n",
      "Train Batch 200 (every 100 batch): Loss=0.1527; accuracy=0.9355\n",
      "Train Batch 300 (every 100 batch): Loss=0.2230; accuracy=0.9328\n",
      "Epoch 45 summary: train_loss: 0.1823 | train_acc: 0.9340 | val_loss: 1.8059 | val_acc: 0.5756\n",
      "Epoch 46\n",
      "Train Batch 100 (every 100 batch): Loss=0.0899; accuracy=0.9344\n",
      "Train Batch 200 (every 100 batch): Loss=0.2350; accuracy=0.9342\n",
      "Train Batch 300 (every 100 batch): Loss=0.3325; accuracy=0.9348\n",
      "Epoch 46 summary: train_loss: 0.1771 | train_acc: 0.9349 | val_loss: 1.6501 | val_acc: 0.5861\n",
      "Epoch 47\n",
      "Train Batch 100 (every 100 batch): Loss=0.2423; accuracy=0.9383\n",
      "Train Batch 200 (every 100 batch): Loss=0.0862; accuracy=0.9390\n",
      "Train Batch 300 (every 100 batch): Loss=0.2922; accuracy=0.9391\n",
      "Epoch 47 summary: train_loss: 0.1729 | train_acc: 0.9379 | val_loss: 1.5732 | val_acc: 0.5934\n",
      "Epoch 48\n",
      "Train Batch 100 (every 100 batch): Loss=0.0875; accuracy=0.9303\n",
      "Train Batch 200 (every 100 batch): Loss=0.1257; accuracy=0.9362\n",
      "Train Batch 300 (every 100 batch): Loss=0.2923; accuracy=0.9367\n",
      "Epoch 48 summary: train_loss: 0.1732 | train_acc: 0.9369 | val_loss: 1.7233 | val_acc: 0.5847\n",
      "Epoch 49\n",
      "Train Batch 100 (every 100 batch): Loss=0.1465; accuracy=0.9308\n",
      "Train Batch 200 (every 100 batch): Loss=0.1580; accuracy=0.9364\n",
      "Train Batch 300 (every 100 batch): Loss=0.2318; accuracy=0.9374\n",
      "Epoch 49 summary: train_loss: 0.1743 | train_acc: 0.9366 | val_loss: 1.6258 | val_acc: 0.5908\n",
      "Epoch 50\n",
      "Train Batch 100 (every 100 batch): Loss=0.2637; accuracy=0.9461\n",
      "Train Batch 200 (every 100 batch): Loss=0.1571; accuracy=0.9454\n",
      "Train Batch 300 (every 100 batch): Loss=0.1930; accuracy=0.9453\n",
      "Epoch 50 summary: train_loss: 0.1580 | train_acc: 0.9447 | val_loss: 1.6597 | val_acc: 0.5894\n",
      "Epoch 51\n",
      "Train Batch 100 (every 100 batch): Loss=0.1255; accuracy=0.9494\n",
      "Train Batch 200 (every 100 batch): Loss=0.1583; accuracy=0.9486\n",
      "Train Batch 300 (every 100 batch): Loss=0.1662; accuracy=0.9477\n",
      "Epoch 51 summary: train_loss: 0.1497 | train_acc: 0.9481 | val_loss: 1.8176 | val_acc: 0.5640\n",
      "Epoch 52\n",
      "Train Batch 100 (every 100 batch): Loss=0.0934; accuracy=0.9369\n",
      "Train Batch 200 (every 100 batch): Loss=0.1881; accuracy=0.9421\n",
      "Train Batch 300 (every 100 batch): Loss=0.1169; accuracy=0.9436\n",
      "Epoch 52 summary: train_loss: 0.1525 | train_acc: 0.9453 | val_loss: 1.8504 | val_acc: 0.5781\n",
      "Epoch 53\n",
      "Train Batch 100 (every 100 batch): Loss=0.0795; accuracy=0.9542\n",
      "Train Batch 200 (every 100 batch): Loss=0.1141; accuracy=0.9510\n",
      "Train Batch 300 (every 100 batch): Loss=0.1371; accuracy=0.9509\n",
      "Epoch 53 summary: train_loss: 0.1421 | train_acc: 0.9501 | val_loss: 1.6602 | val_acc: 0.5810\n",
      "Epoch 54\n",
      "Train Batch 100 (every 100 batch): Loss=0.1252; accuracy=0.9545\n",
      "Train Batch 200 (every 100 batch): Loss=0.1897; accuracy=0.9488\n",
      "Train Batch 300 (every 100 batch): Loss=0.1227; accuracy=0.9477\n",
      "Epoch 54 summary: train_loss: 0.1408 | train_acc: 0.9485 | val_loss: 1.7444 | val_acc: 0.5854\n",
      "Epoch 55\n",
      "Train Batch 100 (every 100 batch): Loss=0.1101; accuracy=0.9578\n",
      "Train Batch 200 (every 100 batch): Loss=0.0965; accuracy=0.9560\n",
      "Train Batch 300 (every 100 batch): Loss=0.2075; accuracy=0.9508\n",
      "Epoch 55 summary: train_loss: 0.1409 | train_acc: 0.9492 | val_loss: 1.7334 | val_acc: 0.5828\n",
      "Epoch 56\n",
      "Train Batch 100 (every 100 batch): Loss=0.0708; accuracy=0.9412\n",
      "Train Batch 200 (every 100 batch): Loss=0.2277; accuracy=0.9443\n",
      "Train Batch 300 (every 100 batch): Loss=0.1967; accuracy=0.9451\n",
      "Epoch 56 summary: train_loss: 0.1506 | train_acc: 0.9461 | val_loss: 1.7605 | val_acc: 0.5850\n",
      "Epoch 57\n",
      "Train Batch 100 (every 100 batch): Loss=0.1561; accuracy=0.9497\n",
      "Train Batch 200 (every 100 batch): Loss=0.1323; accuracy=0.9506\n",
      "Train Batch 300 (every 100 batch): Loss=0.1120; accuracy=0.9522\n",
      "Epoch 57 summary: train_loss: 0.1380 | train_acc: 0.9519 | val_loss: 1.8323 | val_acc: 0.5814\n",
      "Epoch 58\n",
      "Train Batch 100 (every 100 batch): Loss=0.1960; accuracy=0.9542\n",
      "Train Batch 200 (every 100 batch): Loss=0.1685; accuracy=0.9527\n",
      "Train Batch 300 (every 100 batch): Loss=0.0611; accuracy=0.9527\n",
      "Epoch 58 summary: train_loss: 0.1341 | train_acc: 0.9521 | val_loss: 1.8093 | val_acc: 0.5807\n",
      "Epoch 59\n",
      "Train Batch 100 (every 100 batch): Loss=0.0727; accuracy=0.9423\n",
      "Train Batch 200 (every 100 batch): Loss=0.0403; accuracy=0.9487\n",
      "Train Batch 300 (every 100 batch): Loss=0.1963; accuracy=0.9495\n",
      "Epoch 59 summary: train_loss: 0.1405 | train_acc: 0.9493 | val_loss: 1.8922 | val_acc: 0.5690\n",
      "Epoch 60\n",
      "Train Batch 100 (every 100 batch): Loss=0.1185; accuracy=0.9470\n",
      "Train Batch 200 (every 100 batch): Loss=0.1853; accuracy=0.9523\n",
      "Train Batch 300 (every 100 batch): Loss=0.0618; accuracy=0.9536\n",
      "Epoch 60 summary: train_loss: 0.1299 | train_acc: 0.9540 | val_loss: 1.9454 | val_acc: 0.5843\n",
      "Epoch 61\n",
      "Train Batch 100 (every 100 batch): Loss=0.3087; accuracy=0.9605\n",
      "Train Batch 200 (every 100 batch): Loss=0.0899; accuracy=0.9572\n",
      "Train Batch 300 (every 100 batch): Loss=0.2584; accuracy=0.9579\n",
      "Epoch 61 summary: train_loss: 0.1182 | train_acc: 0.9578 | val_loss: 1.8249 | val_acc: 0.5941\n",
      "Epoch 62\n",
      "Train Batch 100 (every 100 batch): Loss=0.0744; accuracy=0.9572\n",
      "Train Batch 200 (every 100 batch): Loss=0.1028; accuracy=0.9576\n",
      "Train Batch 300 (every 100 batch): Loss=0.3250; accuracy=0.9578\n",
      "Epoch 62 summary: train_loss: 0.1191 | train_acc: 0.9570 | val_loss: 1.8925 | val_acc: 0.5807\n",
      "Epoch 63\n",
      "Train Batch 100 (every 100 batch): Loss=0.1257; accuracy=0.9592\n",
      "Train Batch 200 (every 100 batch): Loss=0.1558; accuracy=0.9573\n",
      "Train Batch 300 (every 100 batch): Loss=0.0194; accuracy=0.9576\n",
      "Epoch 63 summary: train_loss: 0.1164 | train_acc: 0.9581 | val_loss: 1.9868 | val_acc: 0.5810\n",
      "Epoch 64\n",
      "Train Batch 100 (every 100 batch): Loss=0.0733; accuracy=0.9594\n",
      "Train Batch 200 (every 100 batch): Loss=0.0256; accuracy=0.9583\n",
      "Train Batch 300 (every 100 batch): Loss=0.1413; accuracy=0.9564\n",
      "Epoch 64 summary: train_loss: 0.1230 | train_acc: 0.9562 | val_loss: 1.9132 | val_acc: 0.5625\n",
      "Epoch 65\n",
      "Train Batch 100 (every 100 batch): Loss=0.2093; accuracy=0.9619\n",
      "Train Batch 200 (every 100 batch): Loss=0.1201; accuracy=0.9629\n",
      "Train Batch 300 (every 100 batch): Loss=0.0363; accuracy=0.9613\n",
      "Epoch 65 summary: train_loss: 0.1101 | train_acc: 0.9612 | val_loss: 1.8621 | val_acc: 0.5807\n",
      "Epoch 66\n",
      "Train Batch 100 (every 100 batch): Loss=0.1303; accuracy=0.9667\n",
      "Train Batch 200 (every 100 batch): Loss=0.0588; accuracy=0.9641\n",
      "Train Batch 300 (every 100 batch): Loss=0.1550; accuracy=0.9632\n",
      "Epoch 66 summary: train_loss: 0.1117 | train_acc: 0.9617 | val_loss: 2.0005 | val_acc: 0.5763\n",
      "Epoch 67\n",
      "Train Batch 100 (every 100 batch): Loss=0.1034; accuracy=0.9552\n",
      "Train Batch 200 (every 100 batch): Loss=0.0659; accuracy=0.9568\n",
      "Train Batch 300 (every 100 batch): Loss=0.0604; accuracy=0.9592\n",
      "Epoch 67 summary: train_loss: 0.1172 | train_acc: 0.9591 | val_loss: 1.9625 | val_acc: 0.5749\n",
      "Epoch 68\n",
      "Train Batch 100 (every 100 batch): Loss=0.0533; accuracy=0.9666\n",
      "Train Batch 200 (every 100 batch): Loss=0.1228; accuracy=0.9658\n",
      "Train Batch 300 (every 100 batch): Loss=0.1739; accuracy=0.9640\n",
      "Epoch 68 summary: train_loss: 0.1086 | train_acc: 0.9624 | val_loss: 1.8089 | val_acc: 0.5832\n",
      "Epoch 69\n",
      "Train Batch 100 (every 100 batch): Loss=0.2799; accuracy=0.9584\n",
      "Train Batch 200 (every 100 batch): Loss=0.0763; accuracy=0.9605\n",
      "Train Batch 300 (every 100 batch): Loss=0.0887; accuracy=0.9594\n",
      "Epoch 69 summary: train_loss: 0.1157 | train_acc: 0.9592 | val_loss: 1.8611 | val_acc: 0.5821\n",
      "Epoch 70\n",
      "Train Batch 100 (every 100 batch): Loss=0.1013; accuracy=0.9583\n",
      "Train Batch 200 (every 100 batch): Loss=0.0585; accuracy=0.9555\n",
      "Train Batch 300 (every 100 batch): Loss=0.1153; accuracy=0.9591\n",
      "Epoch 70 summary: train_loss: 0.1194 | train_acc: 0.9590 | val_loss: 1.9295 | val_acc: 0.5927\n",
      "Epoch 71\n",
      "Train Batch 100 (every 100 batch): Loss=0.0748; accuracy=0.9594\n",
      "Train Batch 200 (every 100 batch): Loss=0.1529; accuracy=0.9609\n",
      "Train Batch 300 (every 100 batch): Loss=0.1639; accuracy=0.9603\n",
      "Epoch 71 summary: train_loss: 0.1165 | train_acc: 0.9593 | val_loss: 1.8477 | val_acc: 0.5814\n",
      "Epoch 72\n",
      "Train Batch 100 (every 100 batch): Loss=0.0323; accuracy=0.9641\n",
      "Train Batch 200 (every 100 batch): Loss=0.0915; accuracy=0.9651\n",
      "Train Batch 300 (every 100 batch): Loss=0.0721; accuracy=0.9652\n",
      "Epoch 72 summary: train_loss: 0.1020 | train_acc: 0.9644 | val_loss: 1.7848 | val_acc: 0.5828\n",
      "Epoch 73\n",
      "Train Batch 100 (every 100 batch): Loss=0.1629; accuracy=0.9670\n",
      "Train Batch 200 (every 100 batch): Loss=0.0517; accuracy=0.9676\n",
      "Train Batch 300 (every 100 batch): Loss=0.0381; accuracy=0.9663\n",
      "Epoch 73 summary: train_loss: 0.0990 | train_acc: 0.9653 | val_loss: 2.0209 | val_acc: 0.5807\n",
      "Epoch 74\n",
      "Train Batch 100 (every 100 batch): Loss=0.2150; accuracy=0.9661\n",
      "Train Batch 200 (every 100 batch): Loss=0.1428; accuracy=0.9666\n",
      "Train Batch 300 (every 100 batch): Loss=0.0855; accuracy=0.9663\n",
      "Epoch 74 summary: train_loss: 0.1002 | train_acc: 0.9668 | val_loss: 1.9865 | val_acc: 0.5749\n",
      "Epoch 75\n",
      "Train Batch 100 (every 100 batch): Loss=0.0509; accuracy=0.9650\n",
      "Train Batch 200 (every 100 batch): Loss=0.0575; accuracy=0.9643\n",
      "Train Batch 300 (every 100 batch): Loss=0.1329; accuracy=0.9640\n",
      "Epoch 75 summary: train_loss: 0.1087 | train_acc: 0.9627 | val_loss: 1.8455 | val_acc: 0.5709\n",
      "Epoch 76\n",
      "Train Batch 100 (every 100 batch): Loss=0.0550; accuracy=0.9530\n",
      "Train Batch 200 (every 100 batch): Loss=0.0403; accuracy=0.9578\n",
      "Train Batch 300 (every 100 batch): Loss=0.0759; accuracy=0.9595\n",
      "Epoch 76 summary: train_loss: 0.1162 | train_acc: 0.9592 | val_loss: 1.9214 | val_acc: 0.5887\n",
      "Epoch 77\n",
      "Train Batch 100 (every 100 batch): Loss=0.0654; accuracy=0.9639\n",
      "Train Batch 200 (every 100 batch): Loss=0.0234; accuracy=0.9644\n",
      "Train Batch 300 (every 100 batch): Loss=0.0657; accuracy=0.9632\n",
      "Epoch 77 summary: train_loss: 0.1031 | train_acc: 0.9629 | val_loss: 1.9343 | val_acc: 0.5745\n",
      "Epoch 78\n",
      "Train Batch 100 (every 100 batch): Loss=0.0637; accuracy=0.9658\n",
      "Train Batch 200 (every 100 batch): Loss=0.0372; accuracy=0.9682\n",
      "Train Batch 300 (every 100 batch): Loss=0.1487; accuracy=0.9679\n",
      "Epoch 78 summary: train_loss: 0.0935 | train_acc: 0.9674 | val_loss: 1.9305 | val_acc: 0.5756\n",
      "Epoch 79\n",
      "Train Batch 100 (every 100 batch): Loss=0.0645; accuracy=0.9603\n",
      "Train Batch 200 (every 100 batch): Loss=0.1002; accuracy=0.9607\n",
      "Train Batch 300 (every 100 batch): Loss=0.0613; accuracy=0.9617\n",
      "Epoch 79 summary: train_loss: 0.1094 | train_acc: 0.9612 | val_loss: 1.9288 | val_acc: 0.5716\n",
      "Epoch 80\n",
      "Train Batch 100 (every 100 batch): Loss=0.0330; accuracy=0.9647\n",
      "Train Batch 200 (every 100 batch): Loss=0.1682; accuracy=0.9666\n",
      "Train Batch 300 (every 100 batch): Loss=0.0585; accuracy=0.9682\n",
      "Epoch 80 summary: train_loss: 0.0955 | train_acc: 0.9679 | val_loss: 1.9218 | val_acc: 0.5810\n",
      "Epoch 81\n",
      "Train Batch 100 (every 100 batch): Loss=0.0428; accuracy=0.9628\n",
      "Train Batch 200 (every 100 batch): Loss=0.1799; accuracy=0.9641\n",
      "Train Batch 300 (every 100 batch): Loss=0.1147; accuracy=0.9659\n",
      "Epoch 81 summary: train_loss: 0.0986 | train_acc: 0.9663 | val_loss: 1.9515 | val_acc: 0.5698\n",
      "Epoch 82\n",
      "Train Batch 100 (every 100 batch): Loss=0.0267; accuracy=0.9675\n",
      "Train Batch 200 (every 100 batch): Loss=0.1721; accuracy=0.9684\n",
      "Train Batch 300 (every 100 batch): Loss=0.0763; accuracy=0.9690\n",
      "Epoch 82 summary: train_loss: 0.0916 | train_acc: 0.9690 | val_loss: 1.9256 | val_acc: 0.5872\n",
      "Epoch 83\n",
      "Train Batch 100 (every 100 batch): Loss=0.0909; accuracy=0.9652\n",
      "Train Batch 200 (every 100 batch): Loss=0.1215; accuracy=0.9667\n",
      "Train Batch 300 (every 100 batch): Loss=0.1358; accuracy=0.9681\n",
      "Epoch 83 summary: train_loss: 0.0902 | train_acc: 0.9681 | val_loss: 1.9864 | val_acc: 0.5701\n",
      "Epoch 84\n",
      "Train Batch 100 (every 100 batch): Loss=0.1129; accuracy=0.9702\n",
      "Train Batch 200 (every 100 batch): Loss=0.1586; accuracy=0.9709\n",
      "Train Batch 300 (every 100 batch): Loss=0.0551; accuracy=0.9695\n",
      "Epoch 84 summary: train_loss: 0.0893 | train_acc: 0.9694 | val_loss: 2.0553 | val_acc: 0.5799\n",
      "Epoch 85\n",
      "Train Batch 100 (every 100 batch): Loss=0.0632; accuracy=0.9719\n",
      "Train Batch 200 (every 100 batch): Loss=0.0844; accuracy=0.9710\n",
      "Train Batch 300 (every 100 batch): Loss=0.0312; accuracy=0.9698\n",
      "Epoch 85 summary: train_loss: 0.0893 | train_acc: 0.9694 | val_loss: 1.9155 | val_acc: 0.5814\n",
      "Epoch 86\n",
      "Train Batch 100 (every 100 batch): Loss=0.1599; accuracy=0.9714\n",
      "Train Batch 200 (every 100 batch): Loss=0.0600; accuracy=0.9691\n",
      "Train Batch 300 (every 100 batch): Loss=0.0472; accuracy=0.9686\n",
      "Epoch 86 summary: train_loss: 0.0905 | train_acc: 0.9683 | val_loss: 1.9413 | val_acc: 0.5934\n",
      "Epoch 87\n",
      "Train Batch 100 (every 100 batch): Loss=0.0221; accuracy=0.9689\n",
      "Train Batch 200 (every 100 batch): Loss=0.0851; accuracy=0.9684\n",
      "Train Batch 300 (every 100 batch): Loss=0.1207; accuracy=0.9702\n",
      "Epoch 87 summary: train_loss: 0.0867 | train_acc: 0.9707 | val_loss: 1.9851 | val_acc: 0.5919\n",
      "Epoch 88\n",
      "Train Batch 100 (every 100 batch): Loss=0.0657; accuracy=0.9717\n",
      "Train Batch 200 (every 100 batch): Loss=0.0999; accuracy=0.9721\n",
      "Train Batch 300 (every 100 batch): Loss=0.0589; accuracy=0.9710\n",
      "Epoch 88 summary: train_loss: 0.0831 | train_acc: 0.9705 | val_loss: 2.1432 | val_acc: 0.5825\n",
      "Epoch 89\n",
      "Train Batch 100 (every 100 batch): Loss=0.0635; accuracy=0.9684\n",
      "Train Batch 200 (every 100 batch): Loss=0.1407; accuracy=0.9661\n",
      "Train Batch 300 (every 100 batch): Loss=0.0510; accuracy=0.9677\n",
      "Epoch 89 summary: train_loss: 0.0899 | train_acc: 0.9687 | val_loss: 1.9937 | val_acc: 0.5977\n",
      "Epoch 90\n",
      "Train Batch 100 (every 100 batch): Loss=0.0376; accuracy=0.9755\n",
      "Train Batch 200 (every 100 batch): Loss=0.0434; accuracy=0.9752\n",
      "Train Batch 300 (every 100 batch): Loss=0.0954; accuracy=0.9728\n",
      "Epoch 90 summary: train_loss: 0.0780 | train_acc: 0.9728 | val_loss: 1.8618 | val_acc: 0.5988\n",
      "Epoch 91\n",
      "Train Batch 100 (every 100 batch): Loss=0.1003; accuracy=0.9703\n",
      "Train Batch 200 (every 100 batch): Loss=0.0869; accuracy=0.9705\n",
      "Train Batch 300 (every 100 batch): Loss=0.0397; accuracy=0.9710\n",
      "Epoch 91 summary: train_loss: 0.0823 | train_acc: 0.9707 | val_loss: 2.0141 | val_acc: 0.6068\n",
      "Epoch 92\n",
      "Train Batch 100 (every 100 batch): Loss=0.0648; accuracy=0.9516\n",
      "Train Batch 200 (every 100 batch): Loss=0.0459; accuracy=0.9593\n",
      "Train Batch 300 (every 100 batch): Loss=0.0315; accuracy=0.9621\n",
      "Epoch 92 summary: train_loss: 0.1089 | train_acc: 0.9626 | val_loss: 1.9972 | val_acc: 0.5872\n",
      "Epoch 93\n",
      "Train Batch 100 (every 100 batch): Loss=0.2378; accuracy=0.9712\n",
      "Train Batch 200 (every 100 batch): Loss=0.0551; accuracy=0.9736\n",
      "Train Batch 300 (every 100 batch): Loss=0.1167; accuracy=0.9728\n",
      "Epoch 93 summary: train_loss: 0.0794 | train_acc: 0.9728 | val_loss: 1.9124 | val_acc: 0.5938\n",
      "Epoch 94\n",
      "Train Batch 100 (every 100 batch): Loss=0.1084; accuracy=0.9734\n",
      "Train Batch 200 (every 100 batch): Loss=0.0325; accuracy=0.9730\n",
      "Train Batch 300 (every 100 batch): Loss=0.1017; accuracy=0.9731\n",
      "Epoch 94 summary: train_loss: 0.0784 | train_acc: 0.9730 | val_loss: 2.0246 | val_acc: 0.5861\n",
      "Epoch 95\n",
      "Train Batch 100 (every 100 batch): Loss=0.0412; accuracy=0.9736\n",
      "Train Batch 200 (every 100 batch): Loss=0.0782; accuracy=0.9739\n",
      "Train Batch 300 (every 100 batch): Loss=0.0446; accuracy=0.9726\n",
      "Epoch 95 summary: train_loss: 0.0827 | train_acc: 0.9719 | val_loss: 1.8741 | val_acc: 0.5999\n",
      "Epoch 96\n",
      "Train Batch 100 (every 100 batch): Loss=0.0796; accuracy=0.9664\n",
      "Train Batch 200 (every 100 batch): Loss=0.1132; accuracy=0.9691\n",
      "Train Batch 300 (every 100 batch): Loss=0.0183; accuracy=0.9704\n",
      "Epoch 96 summary: train_loss: 0.0819 | train_acc: 0.9704 | val_loss: 2.1695 | val_acc: 0.5938\n",
      "Epoch 97\n",
      "Train Batch 100 (every 100 batch): Loss=0.0143; accuracy=0.9750\n",
      "Train Batch 200 (every 100 batch): Loss=0.0431; accuracy=0.9741\n",
      "Train Batch 300 (every 100 batch): Loss=0.1111; accuracy=0.9739\n",
      "Epoch 97 summary: train_loss: 0.0742 | train_acc: 0.9740 | val_loss: 2.1689 | val_acc: 0.5752\n",
      "Epoch 98\n",
      "Train Batch 100 (every 100 batch): Loss=0.0220; accuracy=0.9797\n",
      "Train Batch 200 (every 100 batch): Loss=0.1503; accuracy=0.9770\n",
      "Train Batch 300 (every 100 batch): Loss=0.0625; accuracy=0.9764\n",
      "Epoch 98 summary: train_loss: 0.0695 | train_acc: 0.9766 | val_loss: 2.0489 | val_acc: 0.5825\n",
      "Epoch 99\n",
      "Train Batch 100 (every 100 batch): Loss=0.0546; accuracy=0.9795\n",
      "Train Batch 200 (every 100 batch): Loss=0.0611; accuracy=0.9783\n",
      "Train Batch 300 (every 100 batch): Loss=0.1083; accuracy=0.9762\n",
      "Epoch 99 summary: train_loss: 0.0714 | train_acc: 0.9760 | val_loss: 1.9420 | val_acc: 0.5792\n",
      "Epoch 100\n",
      "Train Batch 100 (every 100 batch): Loss=0.0583; accuracy=0.9755\n",
      "Train Batch 200 (every 100 batch): Loss=0.0886; accuracy=0.9741\n",
      "Train Batch 300 (every 100 batch): Loss=0.0328; accuracy=0.9746\n",
      "Epoch 100 summary: train_loss: 0.0769 | train_acc: 0.9745 | val_loss: 1.9629 | val_acc: 0.5916\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'EEG_Encoder.EEGChannelNet.classifier_EEGChannelNet'>: it's not the same object as EEG_Encoder.EEGChannelNet.classifier_EEGChannelNet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m non_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mkind\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom-scratch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mnet_trainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchannel_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnonclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GithubClonedRepo/EEG-Research/Experiment/EEG_Encoder/net_trainer.py:127\u001b[0m, in \u001b[0;36mnet_trainer\u001b[0;34m(net, loaders, opt, channel_idx, nonclasses, pretrain, train, save, print_every_train, print_every_val)\u001b[0m\n\u001b[1;32m    125\u001b[0m         results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_acc)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "File \u001b[0;32m~/GithubClonedRepo/EEG-Research/.env/lib/python3.10/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/GithubClonedRepo/EEG-Research/.env/lib/python3.10/site-packages/torch/serialization.py:653\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    651\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    652\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 653\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    655\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'EEG_Encoder.EEGChannelNet.classifier_EEGChannelNet'>: it's not the same object as EEG_Encoder.EEGChannelNet.classifier_EEGChannelNet"
     ]
    }
   ],
   "source": [
    "channel_idx=None\n",
    "non_classes=None\n",
    "if opt.kind==\"from-scratch\":\n",
    "    results = net_trainer(\n",
    "            net,\n",
    "            loaders,\n",
    "            opt,\n",
    "            channel_idx,\n",
    "            nonclasses,\n",
    "            None,\n",
    "            True,\n",
    "            model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0199dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val =accuracy_val\n",
    "# test = accuracy_test\n",
    "\n",
    "# print(\"Validation accuracy: \", val)\n",
    "# print(\"Test accuracy: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(results, results_path+'.obj')\n",
    "loaded_results = torch.load(results_path+'.obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f19f5-15b8-42f8-a942-4cb82e95d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = loaded_results['train_loss']\n",
    "val_values = loaded_results['val_loss']\n",
    "print(len(train_values))\n",
    "print(len(val_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 101)\n",
    "print(len(epochs))\n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_values, label='Training Loss')\n",
    "plt.plot(epochs, val_values, label='Validation Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, 101, 10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b02c66-313e-44c9-8a39-dac61f759ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_values = torch.stack(loaded_results['train_acc']).tolist()\n",
    "val_acc_values = torch.stack(loaded_results['val_acc']).tolist()\n",
    "print(len(train_acc_values))\n",
    "print(len(val_acc_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 101)\n",
    "print(len(epochs))\n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_acc_values, label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc_values, label='Validation Accuracy')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, 101, 10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b77fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080e706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17823cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b650c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb911b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b43930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28237288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ded073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcbdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fb11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b75470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8939417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5c258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7de01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597563e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d74a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db30ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1544e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
